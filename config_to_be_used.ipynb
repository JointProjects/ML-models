{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function - Download reqd csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_ret_df(ticker):\n",
    "\n",
    "    data_source = 'alphavantage' # alphavantage or kaggle\n",
    "\n",
    "    if data_source == 'alphavantage':\n",
    "        # ====================== Loading Data from Alpha Vantage ==================================\n",
    "\n",
    "        api_key = '2KD8FZPAQ5VWR9MO'\n",
    "\n",
    "        # American Airlines stock market prices\n",
    "        ticker = ticker\n",
    "\n",
    "        # JSON file with all the stock market data for AAL from the last 20 years\n",
    "        url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
    "\n",
    "        # Save data to this file\n",
    "        file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "\n",
    "        # If you haven't already saved data,\n",
    "        # Go ahead and grab the data from the url\n",
    "        # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "        if os.path.exists(file_to_save):\n",
    "            with urllib.request.urlopen(url_string) as url:\n",
    "                data = json.loads(url.read().decode())\n",
    "                # extract stock market data\n",
    "                data = data['Time Series (Daily)']\n",
    "                df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "                for k,v in data.items():\n",
    "                    date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                    data_row = [date.date(),float(v['3. low']),float(v['2. high']),\n",
    "                                float(v['4. close']),float(v['1. open'])]\n",
    "                    df.loc[-1,:] = data_row\n",
    "                    df.index = df.index + 1\n",
    "            print('Data saved to : %s'%file_to_save)        \n",
    "            df.to_csv(file_to_save)\n",
    "            \n",
    "            return file_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to : stock_market_data-AAL.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.59</td>\n",
       "      <td>35.64</td>\n",
       "      <td>37.46</td>\n",
       "      <td>36.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.3</td>\n",
       "      <td>35.07</td>\n",
       "      <td>36.47</td>\n",
       "      <td>36.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.16</td>\n",
       "      <td>34.81</td>\n",
       "      <td>35.9</td>\n",
       "      <td>35.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.44</td>\n",
       "      <td>32.33</td>\n",
       "      <td>34.78</td>\n",
       "      <td>34.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.14</td>\n",
       "      <td>31.95</td>\n",
       "      <td>33.995</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    open    low    high  close\n",
       "1  36.59  35.64   37.46  36.33\n",
       "2   35.3  35.07   36.47  36.37\n",
       "3  35.16  34.81    35.9  35.08\n",
       "4  32.44  32.33   34.78  34.66\n",
       "5  33.14  31.95  33.995   32.6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_save = download_and_ret_df('AAL')\n",
    "df1 = pd.read_csv(file_to_save,names=['ind','date','low','high','close','open'])\n",
    "df1.head()\n",
    "\n",
    "df1 = df1.drop(axis=0,columns=['ind','date'])\n",
    "df1 = df1.drop(df1.index[0])\n",
    "\n",
    "df1 = df1.reindex(columns=['open','low','high','close'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open_values = df1[\"open\"].values\n",
    "plt.plot(open_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>3291</th>\n",
       "      <th>3292</th>\n",
       "      <th>3293</th>\n",
       "      <th>3294</th>\n",
       "      <th>3295</th>\n",
       "      <th>3296</th>\n",
       "      <th>3297</th>\n",
       "      <th>3298</th>\n",
       "      <th>3299</th>\n",
       "      <th>3300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>36.59</td>\n",
       "      <td>35.30</td>\n",
       "      <td>35.16</td>\n",
       "      <td>32.44</td>\n",
       "      <td>33.140</td>\n",
       "      <td>31.40</td>\n",
       "      <td>31.19</td>\n",
       "      <td>32.48</td>\n",
       "      <td>31.53</td>\n",
       "      <td>32.27</td>\n",
       "      <td>...</td>\n",
       "      <td>22.28</td>\n",
       "      <td>22.25</td>\n",
       "      <td>22.60</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.44</td>\n",
       "      <td>20.90</td>\n",
       "      <td>20.26</td>\n",
       "      <td>20.40</td>\n",
       "      <td>19.30</td>\n",
       "      <td>21.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>35.64</td>\n",
       "      <td>35.07</td>\n",
       "      <td>34.81</td>\n",
       "      <td>32.33</td>\n",
       "      <td>31.950</td>\n",
       "      <td>31.30</td>\n",
       "      <td>31.12</td>\n",
       "      <td>30.24</td>\n",
       "      <td>30.60</td>\n",
       "      <td>31.82</td>\n",
       "      <td>...</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.80</td>\n",
       "      <td>22.40</td>\n",
       "      <td>21.75</td>\n",
       "      <td>21.44</td>\n",
       "      <td>20.90</td>\n",
       "      <td>20.18</td>\n",
       "      <td>20.10</td>\n",
       "      <td>19.20</td>\n",
       "      <td>19.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>37.46</td>\n",
       "      <td>36.47</td>\n",
       "      <td>35.90</td>\n",
       "      <td>34.78</td>\n",
       "      <td>33.995</td>\n",
       "      <td>33.01</td>\n",
       "      <td>33.33</td>\n",
       "      <td>32.75</td>\n",
       "      <td>32.44</td>\n",
       "      <td>32.52</td>\n",
       "      <td>...</td>\n",
       "      <td>22.29</td>\n",
       "      <td>22.60</td>\n",
       "      <td>23.00</td>\n",
       "      <td>22.31</td>\n",
       "      <td>22.50</td>\n",
       "      <td>21.75</td>\n",
       "      <td>21.05</td>\n",
       "      <td>20.58</td>\n",
       "      <td>20.53</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>36.33</td>\n",
       "      <td>36.37</td>\n",
       "      <td>35.08</td>\n",
       "      <td>34.66</td>\n",
       "      <td>32.600</td>\n",
       "      <td>32.46</td>\n",
       "      <td>32.37</td>\n",
       "      <td>30.34</td>\n",
       "      <td>32.38</td>\n",
       "      <td>32.16</td>\n",
       "      <td>...</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.20</td>\n",
       "      <td>22.16</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.01</td>\n",
       "      <td>20.21</td>\n",
       "      <td>20.50</td>\n",
       "      <td>19.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 3300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1      2      3      4       5      6      7      8      9     10  \\\n",
       "open   36.59  35.30  35.16  32.44  33.140  31.40  31.19  32.48  31.53  32.27   \n",
       "low    35.64  35.07  34.81  32.33  31.950  31.30  31.12  30.24  30.60  31.82   \n",
       "high   37.46  36.47  35.90  34.78  33.995  33.01  33.33  32.75  32.44  32.52   \n",
       "close  36.33  36.37  35.08  34.66  32.600  32.46  32.37  30.34  32.38  32.16   \n",
       "\n",
       "       ...     3291   3292   3293   3294   3295   3296   3297   3298   3299  \\\n",
       "open   ...    22.28  22.25  22.60  22.10  21.44  20.90  20.26  20.40  19.30   \n",
       "low    ...    22.10  21.80  22.40  21.75  21.44  20.90  20.18  20.10  19.20   \n",
       "high   ...    22.29  22.60  23.00  22.31  22.50  21.75  21.05  20.58  20.53   \n",
       "close  ...    22.21  22.15  22.58  22.20  22.16  21.50  21.01  20.21  20.50   \n",
       "\n",
       "        3300  \n",
       "open   21.05  \n",
       "low    19.10  \n",
       "high   21.40  \n",
       "close  19.30  \n",
       "\n",
       "[4 rows x 3300 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1.transpose()\n",
    "df.to_csv(\"tester.csv\",float_format=np.float32)\n",
    "df = pd.read_csv(\"tester.csv\",index_col=0)\n",
    "#df.drop(axis=0,columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total = n-1\n",
    "# batch size = 30% of total\n",
    "\n",
    "batch_size = int(0.02*(df.shape[1]-1))\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = int(df.shape[1]/(0.02*(df.shape[1])))\n",
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tf.placeholder(dtype=tf.float32, shape=(None,None))\n",
    "tY = tf.placeholder(dtype=tf.float32, shape=(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = batch_size-1\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 4\n",
    "n_hidden_3 = 4\n",
    "n_out = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1':tf.Variable(np.ndarray.astype(np.random.uniform(high=.2,low=0,size=(n_features,n_hidden_1)),np.float32)),\n",
    "    'h2':tf.Variable(np.ndarray.astype(np.random.uniform(high=.001,low=0,size=(n_hidden_1,n_hidden_2)),np.float32)),\n",
    "    'h3':tf.Variable(np.ndarray.astype(np.random.uniform(high=1,low=0,size=(n_hidden_2,n_hidden_3)),np.float32)),\n",
    "    'out':tf.Variable(np.ndarray.astype(np.random.uniform(high=.001,low=0,size=(n_hidden_3,n_out)),np.float32))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'h1':tf.Variable(tf.ones([n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.zeros([n_hidden_2])),\n",
    "    'h3':tf.Variable(tf.ones([n_hidden_3])),\n",
    "    'out':tf.Variable(tf.ones([n_out]))\n",
    "}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    np.savetxt(\"weights1.csv\",sess.run(weights['h1']),delimiter=',')\n",
    "    np.savetxt(\"weights2.csv\",sess.run(weights['h2']),delimiter=',')\n",
    "    np.savetxt(\"weights3.csv\",sess.run(weights['out']),delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x):\n",
    "    hidden1 =tf.nn.leaky_relu(tf.add(tf.matmul(x,weights['h1']),biases['h1']))\n",
    "    hidden2 =tf.nn.leaky_relu(tf.add(tf.matmul(hidden1,weights['h2']),biases['h2']))\n",
    "    #hidden3 =tf.nn.leaky_relu(tf.add(tf.matmul(hidden2,weights['h3']),biases['h3']))\n",
    "    out_op = tf.nn.leaky_relu(tf.add(tf.matmul(hidden2,weights['out']),biases['out']))\n",
    "    \n",
    "    return out_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = network(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "X_train\n",
      " [[[37.86   39.38   39.26   ... 42.92   41.75   42.25  ]\n",
      "  [37.35   37.69   39.14   ... 42.1235 41.75   41.54  ]\n",
      "  [38.12   39.47   39.87   ... 43.23   43.45   42.49  ]\n",
      "  [37.93   38.14   39.54   ... 42.44   42.86   42.11  ]]\n",
      "\n",
      " [[42.86   43.5    42.55   ... 54.     53.08   52.45  ]\n",
      "  [42.56   42.76   42.325  ... 53.59   53.     52.36  ]\n",
      "  [43.46   43.81   43.48   ... 54.64   54.71   53.05  ]\n",
      "  [43.36   42.93   43.4    ... 53.88   54.32   52.59  ]]\n",
      "\n",
      " [[53.65   54.     54.35   ... 48.41   52.     51.78  ]\n",
      "  [52.34   52.21   53.45   ... 47.33   48.32   50.61  ]\n",
      "  [53.9    54.55   55.46   ... 48.89   53.19   51.78  ]\n",
      "  [53.07   53.05   54.79   ... 47.56   48.61   51.02  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[46.77   44.9    45.     ... 46.65   45.55   45.49  ]\n",
      "  [45.6    44.65   44.36   ... 46.6    45.55   45.1   ]\n",
      "  [47.44   46.78   46.64   ... 47.7    46.75   47.7   ]\n",
      "  [46.23   46.53   44.9    ... 47.47   46.47   45.2   ]]\n",
      "\n",
      " [[41.8    44.15   47.6    ... 35.15   35.2    34.28  ]\n",
      "  [41.29   41.     43.5    ... 34.85   34.8    34.2   ]\n",
      "  [44.17   44.52   47.6    ... 35.43   35.35   35.5   ]\n",
      "  [42.49   41.55   44.25   ... 35.43   35.17   35.07  ]]\n",
      "\n",
      " [[33.25   32.95   33.4    ... 33.8    33.41   33.95  ]\n",
      "  [32.78   32.93   32.6    ... 33.71   33.35   33.53  ]\n",
      "  [33.7    33.53   33.41   ... 33.91   34.07   33.98  ]\n",
      "  [33.05   33.26   32.85   ... 33.76   33.88   33.66  ]]]\n",
      "************\n",
      "X_test\n",
      " [[35.3    35.16   32.44   33.14   31.4    31.19   32.48   31.53   32.27\n",
      "  32.09   33.58   34.45   32.06   31.     31.97   32.3    33.33   36.3\n",
      "  36.44   37.93   38.8    38.7    39.6    41.41   41.37   41.23   40.84\n",
      "  42.     43.27   42.04   41.82   40.79   40.36   39.7    39.54   39.49\n",
      "  40.26   39.33   38.66   38.31   39.48   40.81   40.32   40.25   40.36\n",
      "  40.66   39.67   39.02   38.9    39.19   40.08   39.98   38.04   37.99\n",
      "  37.75   36.96   36.84   37.29   37.75   38.55   38.41   38.65   38.26\n",
      "  38.06  ]\n",
      " [35.07   34.81   32.33   31.95   31.3    31.12   30.24   30.6    31.82\n",
      "  31.81   31.87   33.53   31.98   30.82   30.81   30.75   31.59   33.41\n",
      "  35.6    36.21   37.48   38.42   38.4    39.6    40.7    41.15   40.78\n",
      "  40.27   41.85   41.99   41.19   40.7    39.65   39.7    39.32   39.42\n",
      "  39.23   39.065  38.49   38.15   38.2485 38.88   40.32   40.18   40.1504\n",
      "  40.25   39.6    39.02   38.5272 38.72   39.14   39.84   37.99   37.72\n",
      "  37.44   36.415  36.83   36.68   36.445  37.885  38.22   38.19   37.87\n",
      "  38.05  ]\n",
      " [36.47   35.9    34.78   33.995  33.01   33.33   32.75   32.44   32.52\n",
      "  32.65   33.759  35.4    33.44   32.24   32.13   32.75   33.4    36.39\n",
      "  36.85   38.1275 39.01   39.26   39.6    41.75   41.955  42.2    41.48\n",
      "  42.19   43.3    43.89   42.17   41.7668 40.86   40.545  39.97   40.06\n",
      "  40.401  40.305  39.49   39.27   39.69   41.37   41.34   40.55   40.84\n",
      "  41.04   40.7    39.82   39.15   39.39   40.19   40.71   40.1    38.21\n",
      "  38.455  37.51   37.555  37.33   37.79   38.74   38.86   38.74   38.49\n",
      "  38.62  ]\n",
      " [36.37   35.08   34.66   32.6    32.46   32.37   30.34   32.38   32.16\n",
      "  32.04   32.06   33.57   33.28   31.78   30.91   31.27   31.61   33.55\n",
      "  35.9    36.44   37.92   38.8    38.5    39.61   41.33   41.5    41.04\n",
      "  40.81   41.98   43.6    41.89   41.6    40.79   40.32   39.74   39.48\n",
      "  39.26   40.18   39.43   38.43   38.48   39.67   40.77   40.48   40.4\n",
      "  40.5    40.69   39.52   38.82   38.83   39.19   40.33   39.99   37.79\n",
      "  38.16   37.44   37.09   36.79   37.26   37.92   38.38   38.26   38.41\n",
      "  38.42  ]]\n",
      "************\n",
      "Y_train\n",
      " [[[37.86  ]\n",
      "  [37.35  ]\n",
      "  [38.12  ]\n",
      "  [37.93  ]]\n",
      "\n",
      " [[42.86  ]\n",
      "  [42.56  ]\n",
      "  [43.46  ]\n",
      "  [43.36  ]]\n",
      "\n",
      " [[53.65  ]\n",
      "  [52.34  ]\n",
      "  [53.9   ]\n",
      "  [53.07  ]]\n",
      "\n",
      " [[51.92  ]\n",
      "  [50.87  ]\n",
      "  [51.9499]\n",
      "  [50.98  ]]\n",
      "\n",
      " [[52.13  ]\n",
      "  [51.45  ]\n",
      "  [52.55  ]\n",
      "  [51.91  ]]\n",
      "\n",
      " [[44.07  ]\n",
      "  [43.9   ]\n",
      "  [44.775 ]\n",
      "  [44.4   ]]\n",
      "\n",
      " [[48.16  ]\n",
      "  [47.54  ]\n",
      "  [48.43  ]\n",
      "  [47.65  ]]\n",
      "\n",
      " [[39.99  ]\n",
      "  [38.06  ]\n",
      "  [39.99  ]\n",
      "  [38.2   ]]\n",
      "\n",
      " [[30.23  ]\n",
      "  [30.22  ]\n",
      "  [31.44  ]\n",
      "  [31.16  ]]\n",
      "\n",
      " [[38.66  ]\n",
      "  [37.91  ]\n",
      "  [39.05  ]\n",
      "  [38.36  ]]\n",
      "\n",
      " [[41.29  ]\n",
      "  [40.3   ]\n",
      "  [41.3501]\n",
      "  [40.91  ]]\n",
      "\n",
      " [[39.18  ]\n",
      "  [37.72  ]\n",
      "  [39.68  ]\n",
      "  [38.83  ]]\n",
      "\n",
      " [[40.77  ]\n",
      "  [39.59  ]\n",
      "  [41.32  ]\n",
      "  [39.75  ]]\n",
      "\n",
      " [[50.84  ]\n",
      "  [50.25  ]\n",
      "  [51.685 ]\n",
      "  [51.265 ]]\n",
      "\n",
      " [[49.67  ]\n",
      "  [49.1   ]\n",
      "  [50.94  ]\n",
      "  [50.71  ]]\n",
      "\n",
      " [[37.7   ]\n",
      "  [37.6   ]\n",
      "  [38.11  ]\n",
      "  [38.05  ]]\n",
      "\n",
      " [[41.08  ]\n",
      "  [40.76  ]\n",
      "  [42.19  ]\n",
      "  [41.87  ]]\n",
      "\n",
      " [[36.    ]\n",
      "  [35.67  ]\n",
      "  [37.1799]\n",
      "  [36.34  ]]\n",
      "\n",
      " [[23.95  ]\n",
      "  [23.4501]\n",
      "  [25.44  ]\n",
      "  [24.6   ]]\n",
      "\n",
      " [[17.18  ]\n",
      "  [16.62  ]\n",
      "  [17.2   ]\n",
      "  [16.8   ]]\n",
      "\n",
      " [[17.15  ]\n",
      "  [16.85  ]\n",
      "  [17.62  ]\n",
      "  [16.95  ]]\n",
      "\n",
      " [[13.5   ]\n",
      "  [13.47  ]\n",
      "  [14.07  ]\n",
      "  [13.9   ]]\n",
      "\n",
      " [[12.54  ]\n",
      "  [12.4   ]\n",
      "  [12.92  ]\n",
      "  [12.72  ]]\n",
      "\n",
      " [[11.34  ]\n",
      "  [11.21  ]\n",
      "  [11.73  ]\n",
      "  [11.61  ]]\n",
      "\n",
      " [[10.07  ]\n",
      "  [10.07  ]\n",
      "  [10.75  ]\n",
      "  [10.71  ]]\n",
      "\n",
      " [[ 8.85  ]\n",
      "  [ 8.68  ]\n",
      "  [ 9.16  ]\n",
      "  [ 9.1   ]]\n",
      "\n",
      " [[ 4.91  ]\n",
      "  [ 4.87  ]\n",
      "  [ 5.1   ]\n",
      "  [ 5.01  ]]\n",
      "\n",
      " [[ 5.45  ]\n",
      "  [ 5.25  ]\n",
      "  [ 5.56  ]\n",
      "  [ 5.48  ]]\n",
      "\n",
      " [[ 9.39  ]\n",
      "  [ 9.38  ]\n",
      "  [ 9.8   ]\n",
      "  [ 9.5   ]]\n",
      "\n",
      " [[ 9.5   ]\n",
      "  [ 9.21  ]\n",
      "  [ 9.5   ]\n",
      "  [ 9.24  ]]\n",
      "\n",
      " [[11.75  ]\n",
      "  [11.55  ]\n",
      "  [11.92  ]\n",
      "  [11.92  ]]\n",
      "\n",
      " [[11.    ]\n",
      "  [10.64  ]\n",
      "  [11.4   ]\n",
      "  [10.64  ]]\n",
      "\n",
      " [[ 7.32  ]\n",
      "  [ 7.08  ]\n",
      "  [ 7.45  ]\n",
      "  [ 7.33  ]]\n",
      "\n",
      " [[ 5.21  ]\n",
      "  [ 5.05  ]\n",
      "  [ 5.28  ]\n",
      "  [ 5.05  ]]\n",
      "\n",
      " [[ 4.3   ]\n",
      "  [ 3.82  ]\n",
      "  [ 4.46  ]\n",
      "  [ 3.87  ]]\n",
      "\n",
      " [[ 2.21  ]\n",
      "  [ 2.05  ]\n",
      "  [ 2.24  ]\n",
      "  [ 2.08  ]]\n",
      "\n",
      " [[ 3.81  ]\n",
      "  [ 3.7   ]\n",
      "  [ 4.14  ]\n",
      "  [ 3.97  ]]\n",
      "\n",
      " [[ 8.35  ]\n",
      "  [ 8.11  ]\n",
      "  [ 8.66  ]\n",
      "  [ 8.27  ]]\n",
      "\n",
      " [[ 4.4   ]\n",
      "  [ 3.56  ]\n",
      "  [ 4.74  ]\n",
      "  [ 3.63  ]]\n",
      "\n",
      " [[ 2.7   ]\n",
      "  [ 2.51  ]\n",
      "  [ 2.8   ]\n",
      "  [ 2.61  ]]\n",
      "\n",
      " [[ 9.57  ]\n",
      "  [ 9.27  ]\n",
      "  [ 9.72  ]\n",
      "  [ 9.62  ]]\n",
      "\n",
      " [[14.55  ]\n",
      "  [13.25  ]\n",
      "  [14.58  ]\n",
      "  [13.32  ]]\n",
      "\n",
      " [[26.48  ]\n",
      "  [25.75  ]\n",
      "  [26.53  ]\n",
      "  [26.25  ]]\n",
      "\n",
      " [[28.03  ]\n",
      "  [27.57  ]\n",
      "  [29.77  ]\n",
      "  [29.67  ]]\n",
      "\n",
      " [[47.    ]\n",
      "  [46.39  ]\n",
      "  [47.58  ]\n",
      "  [46.72  ]]\n",
      "\n",
      " [[56.62  ]\n",
      "  [55.25  ]\n",
      "  [57.4   ]\n",
      "  [55.8   ]]\n",
      "\n",
      " [[46.77  ]\n",
      "  [45.6   ]\n",
      "  [47.44  ]\n",
      "  [46.23  ]]\n",
      "\n",
      " [[41.8   ]\n",
      "  [41.29  ]\n",
      "  [44.17  ]\n",
      "  [42.49  ]]\n",
      "\n",
      " [[33.25  ]\n",
      "  [32.78  ]\n",
      "  [33.7   ]\n",
      "  [33.05  ]]]\n",
      "************\n",
      "Y_test\n",
      " [[36.59]\n",
      " [35.64]\n",
      " [37.46]\n",
      " [36.33]]\n",
      "************\n",
      "To Be Predicted\n",
      " [[36.59   35.3    35.16   32.44   33.14   31.4    31.19   32.48   31.53\n",
      "  32.27   32.09   33.58   34.45   32.06   31.     31.97   32.3    33.33\n",
      "  36.3    36.44   37.93   38.8    38.7    39.6    41.41   41.37   41.23\n",
      "  40.84   42.     43.27   42.04   41.82   40.79   40.36   39.7    39.54\n",
      "  39.49   40.26   39.33   38.66   38.31   39.48   40.81   40.32   40.25\n",
      "  40.36   40.66   39.67   39.02   38.9    39.19   40.08   39.98   38.04\n",
      "  37.99   37.75   36.96   36.84   37.29   37.75   38.55   38.41   38.65\n",
      "  38.26  ]\n",
      " [35.64   35.07   34.81   32.33   31.95   31.3    31.12   30.24   30.6\n",
      "  31.82   31.81   31.87   33.53   31.98   30.82   30.81   30.75   31.59\n",
      "  33.41   35.6    36.21   37.48   38.42   38.4    39.6    40.7    41.15\n",
      "  40.78   40.27   41.85   41.99   41.19   40.7    39.65   39.7    39.32\n",
      "  39.42   39.23   39.065  38.49   38.15   38.2485 38.88   40.32   40.18\n",
      "  40.1504 40.25   39.6    39.02   38.5272 38.72   39.14   39.84   37.99\n",
      "  37.72   37.44   36.415  36.83   36.68   36.445  37.885  38.22   38.19\n",
      "  37.87  ]\n",
      " [37.46   36.47   35.9    34.78   33.995  33.01   33.33   32.75   32.44\n",
      "  32.52   32.65   33.759  35.4    33.44   32.24   32.13   32.75   33.4\n",
      "  36.39   36.85   38.1275 39.01   39.26   39.6    41.75   41.955  42.2\n",
      "  41.48   42.19   43.3    43.89   42.17   41.7668 40.86   40.545  39.97\n",
      "  40.06   40.401  40.305  39.49   39.27   39.69   41.37   41.34   40.55\n",
      "  40.84   41.04   40.7    39.82   39.15   39.39   40.19   40.71   40.1\n",
      "  38.21   38.455  37.51   37.555  37.33   37.79   38.74   38.86   38.74\n",
      "  38.49  ]\n",
      " [36.33   36.37   35.08   34.66   32.6    32.46   32.37   30.34   32.38\n",
      "  32.16   32.04   32.06   33.57   33.28   31.78   30.91   31.27   31.61\n",
      "  33.55   35.9    36.44   37.92   38.8    38.5    39.61   41.33   41.5\n",
      "  41.04   40.81   41.98   43.6    41.89   41.6    40.79   40.32   39.74\n",
      "  39.48   39.26   40.18   39.43   38.43   38.48   39.67   40.77   40.48\n",
      "  40.4    40.5    40.69   39.52   38.82   38.83   39.19   40.33   39.99\n",
      "  37.79   38.16   37.44   37.09   36.79   37.26   37.92   38.38   38.26\n",
      "  38.41  ]]\n"
     ]
    }
   ],
   "source": [
    "def splitter(df,batch_size,total_size):\n",
    "    \n",
    "    start = 0\n",
    "    \n",
    "    X_test = df.iloc(axis=1)[0:, start+1:batch_size].values\n",
    "    \n",
    "    Y_test = df.iloc(axis=1)[0:, start:start+1].values\n",
    "    \n",
    "    \n",
    "    #X_test,Y_test = converter_values_float(X_test,Y_test)\n",
    "    \n",
    "    #print(\"X_test matrix\\n\", X_test.shape,\"\\n\",X_test)\n",
    "    #print(\"Y_test matrix\\n\", Y_test.shape,\"\\n\",Y_test)\n",
    "    \n",
    "    rem = df.shape[1]-batch_size\n",
    "    #print(\"Rem = \",rem)\n",
    "    # remaining data will be treated as train\n",
    "    #print(df.iloc[0:,batch_size:])\n",
    "    \n",
    "    X_train_list = list()\n",
    "    Y_train_list = list()\n",
    "    \n",
    "    for i in range(int(rem//batch_size)):\n",
    "        \n",
    "        start2 = batch_size\n",
    "        extend = start2*(i+1) + batch_size\n",
    "        X_train = df.iloc[0:,(start2)*(i+1):extend-1].values\n",
    "        \n",
    "        X_train_list.append(X_train)\n",
    "        \n",
    "        Y_train = df.iloc[0:, start2*(i+1):start2*(i+1)+1].values\n",
    "        \n",
    "        Y_train_list.append(Y_train)\n",
    "        \n",
    "    #print(X_train_list)\n",
    "    #print(Y_train_list)\n",
    "    #convert_train_values_float(X_train_list,Y_train_list)\n",
    "    \n",
    "    \n",
    "    return (X_train_list,X_test, Y_train_list,Y_test)\n",
    "    \n",
    "    \n",
    "\n",
    "total_size = df.shape[1]\n",
    "X_train,X_test, Y_train,Y_test=splitter(df,batch_size,total_size)\n",
    "X_test_final = df.iloc[0:,0:batch_size-1].values\n",
    "\n",
    "\n",
    "#Y,X test\n",
    "#Y,X train\n",
    "X_train = np.asarray(X_train,dtype=np.float32)\n",
    "Y_train = np.asarray(Y_train,dtype=np.float32)\n",
    "print(\"************\\nX_train\\n\",X_train)\n",
    "print(\"************\\nX_test\\n\",X_test)\n",
    "print(\"************\\nY_train\\n\",Y_train)\n",
    "print(\"************\\nY_test\\n\",Y_test)\n",
    "print(\"************\\nTo Be Predicted\\n\",X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_op = cost = tf.reduce_sum(tf.pow(logits-tY, 2))/(2*batch_size)\n",
    "loss_op = tf.reduce_sum(tf.pow(logits-tY, 2))/(2*batch_size)\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  1  LOSS:  41.635853\n",
      "EPOCH:  2  LOSS:  10837.548\n",
      "EPOCH:  3  LOSS:  11.347628\n",
      "EPOCH:  4  LOSS:  60.382336\n",
      "EPOCH:  5  LOSS:  15.458859\n",
      "EPOCH:  6  LOSS:  0.27142\n",
      "EPOCH:  7  LOSS:  1.7809771\n",
      "EPOCH:  8  LOSS:  12.872153\n",
      "EPOCH:  9  LOSS:  25.690859\n",
      "EPOCH:  10  LOSS:  72.50051\n",
      "EPOCH:  11  LOSS:  120.89037\n",
      "EPOCH:  12  LOSS:  152.26027\n",
      "EPOCH:  13  LOSS:  239.88567\n",
      "EPOCH:  14  LOSS:  418.34396\n",
      "EPOCH:  15  LOSS:  379.17438\n",
      "EPOCH:  16  LOSS:  330.36862\n",
      "EPOCH:  17  LOSS:  343.83643\n",
      "EPOCH:  18  LOSS:  248.61531\n",
      "EPOCH:  19  LOSS:  108.85026\n",
      "EPOCH:  20  LOSS:  59.742306\n",
      "EPOCH:  21  LOSS:  53.52166\n",
      "EPOCH:  22  LOSS:  32.58493\n",
      "EPOCH:  23  LOSS:  21.682938\n",
      "EPOCH:  24  LOSS:  19.185701\n",
      "EPOCH:  25  LOSS:  9.5843\n",
      "EPOCH:  26  LOSS:  5.015544\n",
      "EPOCH:  27  LOSS:  2.0098252\n",
      "EPOCH:  28  LOSS:  2.9559538\n",
      "EPOCH:  29  LOSS:  4.1299767\n",
      "EPOCH:  30  LOSS:  3.0335886\n",
      "EPOCH:  31  LOSS:  1.8006481\n",
      "EPOCH:  32  LOSS:  0.62987334\n",
      "EPOCH:  33  LOSS:  0.0023773771\n",
      "EPOCH:  34  LOSS:  0.29284748\n",
      "EPOCH:  35  LOSS:  0.67332923\n",
      "EPOCH:  36  LOSS:  0.6061096\n",
      "EPOCH:  37  LOSS:  0.7937075\n",
      "EPOCH:  38  LOSS:  2.8471172\n",
      "EPOCH:  39  LOSS:  4.472456\n",
      "EPOCH:  40  LOSS:  2.013501\n",
      "EPOCH:  41  LOSS:  6.265692\n",
      "EPOCH:  42  LOSS:  20.245745\n",
      "EPOCH:  43  LOSS:  20.606415\n",
      "EPOCH:  44  LOSS:  1.6936246\n",
      "EPOCH:  45  LOSS:  13.047065\n",
      "EPOCH:  46  LOSS:  17.474432\n",
      "EPOCH:  47  LOSS:  6.730917\n",
      "EPOCH:  48  LOSS:  15.9225025\n",
      "EPOCH:  49  LOSS:  5.9259033\n",
      "0.0023773771\n",
      "[[47.55098 ]\n",
      " [46.35507 ]\n",
      " [48.695705]\n",
      " [47.38426 ]]\n",
      "PREDICTED : [47.55098] ACTUAL:  [36.59]  ACCURACY:  70.04378350845091\n",
      "PREDICTED : [46.35507] ACTUAL:  [35.64]  ACCURACY:  69.9352743917309\n",
      "PREDICTED : [48.695705] ACTUAL:  [37.46]  ACCURACY:  70.00612543027667\n",
      "PREDICTED : [47.38426] ACTUAL:  [36.33]  ACCURACY:  69.57264445289383\n",
      "FINAL PREDICTION\n",
      " [[47.659298]\n",
      " [46.618076]\n",
      " [48.863426]\n",
      " [47.56739 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c7c09fe10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_array = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # train\n",
    "    \n",
    "    for i in range(epoch-1):\n",
    "        _,l = sess.run([optimiser,loss_op],feed_dict={tX:X_train[i],tY:Y_train[i]})\n",
    "        # find proper loss function\n",
    "        print(\"EPOCH: \",i+1,\" LOSS: \",l)\n",
    "        loss_array.append(l)\n",
    "    print(min(loss_array))\n",
    "    # test\n",
    "    final_op_test = logits.eval(feed_dict={tX:X_test})\n",
    "    print(final_op_test)\n",
    "    \n",
    "    # accuracy\n",
    "    accuracy = []\n",
    "    deviation = np.absolute(Y_test-final_op_test)\n",
    "    for j in range(4):\n",
    "        accuracy.append(float(100-(deviation[j]/Y_test[j])*100))\n",
    "    \n",
    "        print(\"PREDICTED :\",final_op_test[j],\"ACTUAL: \",Y_test[j],\" ACCURACY: \",accuracy[j])\n",
    "        \n",
    "    \n",
    "\n",
    "    # final prediction batch\n",
    "    final_pred = logits.eval(feed_dict={tX:X_test_final})\n",
    "    print(\"FINAL PREDICTION\\n\",final_pred)\n",
    "plt.plot(loss_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
