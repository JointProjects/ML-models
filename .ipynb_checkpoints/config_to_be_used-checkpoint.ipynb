{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function - Download reqd csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_ret_df(ticker):\n",
    "\n",
    "    data_source = 'alphavantage' # alphavantage or kaggle\n",
    "\n",
    "    if data_source == 'alphavantage':\n",
    "        # ====================== Loading Data from Alpha Vantage ==================================\n",
    "\n",
    "        api_key = '2KD8FZPAQ5VWR9MO'\n",
    "\n",
    "        # American Airlines stock market prices\n",
    "        ticker = ticker\n",
    "\n",
    "        # JSON file with all the stock market data for AAL from the last 20 years\n",
    "        url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
    "\n",
    "        # Save data to this file\n",
    "        file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "\n",
    "        # If you haven't already saved data,\n",
    "        # Go ahead and grab the data from the url\n",
    "        # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "        if os.path.exists(file_to_save):\n",
    "            with urllib.request.urlopen(url_string) as url:\n",
    "                data = json.loads(url.read().decode())\n",
    "                # extract stock market data\n",
    "                data = data['Time Series (Daily)']\n",
    "                df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "                for k,v in data.items():\n",
    "                    date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                    data_row = [date.date(),float(v['3. low']),float(v['2. high']),\n",
    "                                float(v['4. close']),float(v['1. open'])]\n",
    "                    df.loc[-1,:] = data_row\n",
    "                    df.index = df.index + 1\n",
    "            print('Data saved to : %s'%file_to_save)        \n",
    "            df.to_csv(file_to_save)\n",
    "            \n",
    "            return file_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ad99422df299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_ret_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AAL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_to_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ind'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'low'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'high'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'open'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ind'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlai/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlai/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 424\u001b[0;31m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlai/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Invalid file path or buffer object type: {_type}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "file_to_save = download_and_ret_df('AAL')\n",
    "df1 = pd.read_csv(file_to_save,names=['ind','date','low','high','close','open'])\n",
    "df1.head()\n",
    "\n",
    "df1 = df1.drop(axis=0,columns=['ind','date'])\n",
    "df1 = df1.drop(df1.index[0])\n",
    "\n",
    "df1 = df1.reindex(columns=['open','low','high','close'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open_values = df1[\"open\"].values\n",
    "plt.plot(open_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.transpose()\n",
    "df.to_csv(\"tester.csv\",float_format=np.float32)\n",
    "df = pd.read_csv(\"tester.csv\",index_col=0)\n",
    "#df.drop(axis=0,columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = n-1\n",
    "# batch size = 30% of total\n",
    "\n",
    "batch_size = int(0.02*(df.shape[1]-1))\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = int(df.shape[1]/(0.02*(df.shape[1])))\n",
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tf.placeholder(dtype=tf.float32, shape=(None,None))\n",
    "tY = tf.placeholder(dtype=tf.float32, shape=(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = batch_size-1\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 4\n",
    "n_hidden_3 = 4\n",
    "n_out = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1':tf.Variable(np.ndarray.astype(np.random.uniform(high=.2,low=0,size=(n_features,n_hidden_1)),np.float32)),\n",
    "    'h2':tf.Variable(np.ndarray.astype(np.random.uniform(high=.001,low=0,size=(n_hidden_1,n_hidden_2)),np.float32)),\n",
    "    'h3':tf.Variable(np.ndarray.astype(np.random.uniform(high=1,low=0,size=(n_hidden_2,n_hidden_3)),np.float32)),\n",
    "    'out':tf.Variable(np.ndarray.astype(np.random.uniform(high=.001,low=0,size=(n_hidden_3,n_out)),np.float32))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'h1':tf.Variable(tf.ones([n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.zeros([n_hidden_2])),\n",
    "    'h3':tf.Variable(tf.ones([n_hidden_3])),\n",
    "    'out':tf.Variable(tf.ones([n_out]))\n",
    "}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    np.savetxt(\"weights1.csv\",sess.run(weights['h1']),delimiter=',')\n",
    "    np.savetxt(\"weights2.csv\",sess.run(weights['h2']),delimiter=',')\n",
    "    np.savetxt(\"weights3.csv\",sess.run(weights['out']),delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x):\n",
    "    hidden1 =tf.nn.leaky_relu(tf.add(tf.matmul(x,weights['h1']),biases['h1']))\n",
    "    hidden2 =tf.nn.leaky_relu(tf.add(tf.matmul(hidden1,weights['h2']),biases['h2']))\n",
    "    #hidden3 =tf.nn.leaky_relu(tf.add(tf.matmul(hidden2,weights['h3']),biases['h3']))\n",
    "    out_op = tf.nn.leaky_relu(tf.add(tf.matmul(hidden2,weights['out']),biases['out']))\n",
    "    \n",
    "    return out_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = network(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(df,batch_size,total_size):\n",
    "    \n",
    "    start = 0\n",
    "    \n",
    "    X_test = df.iloc(axis=1)[0:, start+1:batch_size].values\n",
    "    \n",
    "    Y_test = df.iloc(axis=1)[0:, start:start+1].values\n",
    "    \n",
    "    \n",
    "    #X_test,Y_test = converter_values_float(X_test,Y_test)\n",
    "    \n",
    "    #print(\"X_test matrix\\n\", X_test.shape,\"\\n\",X_test)\n",
    "    #print(\"Y_test matrix\\n\", Y_test.shape,\"\\n\",Y_test)\n",
    "    \n",
    "    rem = df.shape[1]-batch_size\n",
    "    #print(\"Rem = \",rem)\n",
    "    # remaining data will be treated as train\n",
    "    #print(df.iloc[0:,batch_size:])\n",
    "    \n",
    "    X_train_list = list()\n",
    "    Y_train_list = list()\n",
    "    \n",
    "    for i in range(int(rem//batch_size)):\n",
    "        \n",
    "        start2 = batch_size\n",
    "        extend = start2*(i+1) + batch_size\n",
    "        X_train = df.iloc[0:,(start2)*(i+1):extend-1].values\n",
    "        \n",
    "        X_train_list.append(X_train)\n",
    "        \n",
    "        Y_train = df.iloc[0:, start2*(i+1):start2*(i+1)+1].values\n",
    "        \n",
    "        Y_train_list.append(Y_train)\n",
    "        \n",
    "    #print(X_train_list)\n",
    "    #print(Y_train_list)\n",
    "    #convert_train_values_float(X_train_list,Y_train_list)\n",
    "    \n",
    "    \n",
    "    return (X_train_list,X_test, Y_train_list,Y_test)\n",
    "    \n",
    "    \n",
    "\n",
    "total_size = df.shape[1]\n",
    "X_train,X_test, Y_train,Y_test=splitter(df,batch_size,total_size)\n",
    "X_test_final = df.iloc[0:,0:batch_size-1].values\n",
    "\n",
    "\n",
    "#Y,X test\n",
    "#Y,X train\n",
    "X_train = np.asarray(X_train,dtype=np.float32)\n",
    "Y_train = np.asarray(Y_train,dtype=np.float32)\n",
    "print(\"************\\nX_train\\n\",X_train)\n",
    "print(\"************\\nX_test\\n\",X_test)\n",
    "print(\"************\\nY_train\\n\",Y_train)\n",
    "print(\"************\\nY_test\\n\",Y_test)\n",
    "print(\"************\\nTo Be Predicted\\n\",X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_op = cost = tf.reduce_sum(tf.pow(logits-tY, 2))/(2*batch_size)\n",
    "loss_op = tf.reduce_sum(tf.pow(logits-tY, 2))/(2*batch_size)\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # train\n",
    "    \n",
    "    for i in range(epoch-1):\n",
    "        _,l = sess.run([optimiser,loss_op],feed_dict={tX:X_train[i],tY:Y_train[i]})\n",
    "        # find proper loss function\n",
    "        print(\"EPOCH: \",i+1,\" LOSS: \",l)\n",
    "        loss_array.append(l)\n",
    "    print(min(loss_array))\n",
    "    # test\n",
    "    final_op_test = logits.eval(feed_dict={tX:X_test})\n",
    "    print(final_op_test)\n",
    "    \n",
    "    # accuracy\n",
    "    accuracy = []\n",
    "    deviation = np.absolute(Y_test-final_op_test)\n",
    "    for j in range(4):\n",
    "        accuracy.append(float(100-(deviation[j]/Y_test[j])*100))\n",
    "    \n",
    "        print(\"PREDICTED :\",final_op_test[j],\"ACTUAL: \",Y_test[j],\" ACCURACY: \",accuracy[j])\n",
    "        \n",
    "    \n",
    "\n",
    "    # final prediction batch\n",
    "    final_pred = logits.eval(feed_dict={tX:X_test_final})\n",
    "    print(\"FINAL PREDICTION\\n\",final_pred)\n",
    "plt.plot(loss_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
